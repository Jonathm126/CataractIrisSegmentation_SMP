{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "# Assuming you have a model and input tensor\n",
    "model = torch.load('your_model.pth')  # Load your model\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Example input tensor\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a timer\n",
    "timer = Timer()\n",
    "\n",
    "# Measure inference time\n",
    "with torch.no_grad():\n",
    "    timer.start()\n",
    "    output = model(input_tensor)\n",
    "    timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "#### Background - Recap\n",
    "The goal of this work was to evaluate different models for Iris segmentation, as part of the computer vision stack of a robotic cataract system developed by Forsight Robotics. The requirements from this model are as following:\n",
    "\n",
    "* Recieve full-HD, stereo, coaxial video of the surgical field, including: the eye, tools, speculum and other obstructions. \n",
    "* Segment the *limbus* of the eye - the bor\n",
    "* Segmentation must be robust to overcome obstructions due to tools, changes in illumination, motion of the anatomy during surgery, and reflections.\n",
    "* The model should be computationally inexpensive to allow real-time inference at 60 Hz.\n",
    "  \n",
    "<img src=\"report\\eye_anatomy.jpeg\" alt=\"Eye\" style=\"width: 25%\"/>\n",
    "\n",
    "The background and clinical need is explained in more detail in the mid-project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "#### Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluation Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code explanation\n",
    "Th1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms Evaluation\n",
    "`Transforms_v0` vs `Transforms_v1` using Unet with ResNet 34 encoder.\n",
    "\n",
    "Average metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Evaluation\n",
    "`DiceLoss` vs `EdgeDice` using Unet with ResNet 34 encoder.\n",
    "\n",
    "Average metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This notebook will evaluate models trained from the previous parts.\n",
    "\n",
    "\n",
    "### 1. Training convergance\n",
    "\n",
    "We will look at the loss of the model vs. epoch to determine how many epochs (more or less) are needed for training. Unet with ResNet34 pre-trained encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Effect of loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interim conclusions\n",
    "* what loss\n",
    "* transform?\n",
    "\n",
    "### 4. Model comparison\n",
    "Comapre models: \n",
    "* inference time on test images\n",
    "* histograms\n",
    "* metrics\n",
    "\n",
    "#### Qualitative comparison111"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
